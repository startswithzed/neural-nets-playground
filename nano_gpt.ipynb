{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "!pip install -Uqq torch\n",
    "!pip install -Uqq numpy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Raw Implementation Of GPT Like Model\n",
    "### Download The Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-07 12:13:14--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1115394 (1.1M) [text/plain]\r\n",
      "Saving to: ‘input.txt.1’\r\n",
      "\r\n",
      "input.txt.1         100%[===================>]   1.06M   914KB/s    in 1.2s    \r\n",
      "\r\n",
      "2023-03-07 12:13:17 (914 KB/s) - ‘input.txt.1’ saved [1115394/1115394]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "with open('data/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect The Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "1115394"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first 1000 characters\n",
    "print(text[:1000])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get The Vocabulary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    },
    {
     "data": {
      "text/plain": "65"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get set of all chars in the text and then get that as a sorted list\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "vocab_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize The Input\n",
    "Since this is a character level language model, we'll just translate individual characters to integers.\n",
    "\n",
    "Other tokenizers to look into:\n",
    "1. SentencePiece (Google)\n",
    "2. Tiktoken (OpenAI)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 2]\n",
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # take a string and output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # take a list of integers and output a string\n",
    "\n",
    "print(encode(\"Hello World!\"))\n",
    "print(decode(encode(\"Hello World!\")))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "dict"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stoi is a lookup table where key is the index and value is the character\n",
    "type(stoi)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encode the dataset and get a tensor\n",
    "# data type is int16 because our vocab size is only 65\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Training and Validation Splits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "(1003854, 111540)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(0.9*len(data)) # split 90% of data\n",
    "train_data = data[:n] # first 90% is training data\n",
    "val_data = data[n:] # rest is validation data\n",
    "\n",
    "len(train_data), len(val_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Batches Of Data To Train The Model\n",
    "\n",
    "Sample random chunks of data from the training set. These chunks are of fixed max length.\n",
    "In a chunk of 9 characters like `[18, 47, 56, 57, 58,  1, 15, 47, 58]` there are 8 examples for the model to train itself on like:\n",
    "1. In the context of 18, 47 likely comes next.\n",
    "2. In the context of 18 and 47, 56 likely comes next and so on.\n",
    "\n",
    "This also helps the transformer network get used to seeing context length of 1 character upto the max context length."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # max length of chunks\n",
    "train_data[:block_size + 1] # first 9 chars in the training set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: tensor([18])\ttarget: 47\n",
      "input: tensor([18, 47])\ttarget: 56\n",
      "input: tensor([18, 47, 56])\ttarget: 57\n",
      "input: tensor([18, 47, 56, 57])\ttarget: 58\n",
      "input: tensor([18, 47, 56, 57, 58])\ttarget: 1\n",
      "input: tensor([18, 47, 56, 57, 58,  1])\ttarget: 15\n",
      "input: tensor([18, 47, 56, 57, 58,  1, 15])\ttarget: 47\n",
      "input: tensor([18, 47, 56, 57, 58,  1, 15, 47])\ttarget: 58\n"
     ]
    }
   ],
   "source": [
    "# x are the inputs to the transformer\n",
    "x = train_data[:block_size]\n",
    "# y is the next block\n",
    "y = train_data[1:block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] # all chars of x upto t incl. t\n",
    "    target = y[t]\n",
    "    print(f'input: {context}\\ttarget: {target}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# add batching to process multiple inputs simultaneously\n",
    "batch_size = 4 # number of independent sequences to be processed parallely\n",
    "block_size = 8 # max length of the context\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # generate batch_size number of random offsets in the dataset\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack converts multiple rows into a list of rows\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x,y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[56,  6,  1, 58, 53,  1, 53, 44],\n",
      "        [58, 57, 43, 50, 44, 10,  0, 20],\n",
      "        [ 0, 32, 53,  1, 63, 53, 59, 56],\n",
      "        [53,  1, 41, 53, 52, 41, 43, 39]])\n",
      "inputs_shape: torch.Size([4, 8])\n",
      "\n",
      "inputs: tensor([[ 6,  1, 58, 53,  1, 53, 44, 44],\n",
      "        [57, 43, 50, 44, 10,  0, 20, 39],\n",
      "        [32, 53,  1, 63, 53, 59, 56,  1],\n",
      "        [ 1, 41, 53, 52, 41, 43, 39, 50]])\n",
      "inputs_shape: torch.Size([4, 8])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example batches\n",
    "xb, yb = get_batch('train')\n",
    "print(f'inputs: {xb}\\ninputs_shape: {xb.shape}\\n')\n",
    "print(f'inputs: {yb}\\ninputs_shape: {yb.shape}\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [56]\ttarget: 6\n",
      "input: [56, 6]\ttarget: 1\n",
      "input: [56, 6, 1]\ttarget: 58\n",
      "input: [56, 6, 1, 58]\ttarget: 53\n",
      "input: [56, 6, 1, 58, 53]\ttarget: 1\n",
      "input: [56, 6, 1, 58, 53, 1]\ttarget: 53\n",
      "input: [56, 6, 1, 58, 53, 1, 53]\ttarget: 44\n",
      "input: [56, 6, 1, 58, 53, 1, 53, 44]\ttarget: 44\n",
      "input: [58]\ttarget: 57\n",
      "input: [58, 57]\ttarget: 43\n",
      "input: [58, 57, 43]\ttarget: 50\n",
      "input: [58, 57, 43, 50]\ttarget: 44\n",
      "input: [58, 57, 43, 50, 44]\ttarget: 10\n",
      "input: [58, 57, 43, 50, 44, 10]\ttarget: 0\n",
      "input: [58, 57, 43, 50, 44, 10, 0]\ttarget: 20\n",
      "input: [58, 57, 43, 50, 44, 10, 0, 20]\ttarget: 39\n",
      "input: [0]\ttarget: 32\n",
      "input: [0, 32]\ttarget: 53\n",
      "input: [0, 32, 53]\ttarget: 1\n",
      "input: [0, 32, 53, 1]\ttarget: 63\n",
      "input: [0, 32, 53, 1, 63]\ttarget: 53\n",
      "input: [0, 32, 53, 1, 63, 53]\ttarget: 59\n",
      "input: [0, 32, 53, 1, 63, 53, 59]\ttarget: 56\n",
      "input: [0, 32, 53, 1, 63, 53, 59, 56]\ttarget: 1\n",
      "input: [53]\ttarget: 1\n",
      "input: [53, 1]\ttarget: 41\n",
      "input: [53, 1, 41]\ttarget: 53\n",
      "input: [53, 1, 41, 53]\ttarget: 52\n",
      "input: [53, 1, 41, 53, 52]\ttarget: 41\n",
      "input: [53, 1, 41, 53, 52, 41]\ttarget: 43\n",
      "input: [53, 1, 41, 53, 52, 41, 43]\ttarget: 39\n",
      "input: [53, 1, 41, 53, 52, 41, 43, 39]\ttarget: 50\n"
     ]
    }
   ],
   "source": [
    "# input and target mapping for batches\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'input: {context.tolist()}\\ttarget: {target}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bigram Language Model As A Baseline Model\n",
    "\n",
    "Right now we're only predicting what comes next based on just the individual identity of a single token. This is because the tokens aren't aware of each other. They can only see themselves. So we're only making predictions based on what the actual token is.\n",
    "\n",
    "Notice that in the implementation of generate method even though we pass a sequence of characters as context, the Bigram model only looks at the last character in the sequence to make predictions for the next character. The generate method accepts a sequence as context to keep it general."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # logits are basically the scores for the next character in a sequence\n",
    "        # channel means all the possible tokens (here chars) you can have\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) Batch, Time, Channel\n",
    "\n",
    "        # loss function\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # logits need to be reshaped because cross_entropy expects channels as the second dimension\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            # cross_entropy calculates loss a -log likelihood: -ln(char/65)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # idx is the current context of some characters\n",
    "    # generate function extends the input (B, T) to B by T+1, T+2 and so on...\n",
    "    # and continues to do so for max_new_tokens\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, _ = self(idx)\n",
    "            # focus only on the last element in the time dimension\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample from the probability distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([[ 0.1946,  0.0331, -0.9815,  ..., -1.3514, -0.4478, -0.0752],\n",
      "        [-0.5969,  0.6243, -0.9466,  ...,  0.4586,  2.0705, -0.0910],\n",
      "        [-0.5538, -0.4375,  0.5077,  ..., -0.9778,  0.4818, -0.8837],\n",
      "        ...,\n",
      "        [-0.5852,  0.8824, -0.8663,  ...,  0.4530, -0.3067,  0.1469],\n",
      "        [ 1.1357, -1.9022,  1.6090,  ..., -0.4562, -0.8066, -0.3265],\n",
      "        [-1.1815, -1.4287, -1.3938,  ...,  0.1900,  0.7732, -0.0522]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "loss: 4.658012866973877\n",
      "torch.Size([32, 65])\n"
     ]
    }
   ],
   "source": [
    "# example prediction\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(f'logits: {logits}\\nloss: {loss}\\n{logits.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AmzHN3vnh&3gjHaC.WGZukA'NHUXGWP NfeNL.&K;ids q$MZUXvC\n",
      "-kzdWD ywuhox&j-$UXaSp&j:IAsuAAl;cayMGk-HlQ?s'\n"
     ]
    }
   ],
   "source": [
    "# example generation\n",
    "# B = 1 and T = 1 to kick off the generation\n",
    "# 0 is also encoded as \\n, which is a good place to start\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist())) #[0] to get a single batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
