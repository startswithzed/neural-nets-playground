{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Multi Layer Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('../data/names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build vocab and mappings\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "chtoi ={ch:i+1 for i, ch in enumerate(chars)} # char to int mapping\n",
    "chtoi['.'] = 0 # add separator\n",
    "itoch = {i:ch for ch, i in chtoi.items()} # int to char mapping\n",
    "print(itoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the dataset\n",
    "block_size = 3 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`block_size` represents the context length i.e. how many characters does the model take to predict the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... -> e\n",
      "..e -> m\n",
      ".em -> m\n",
      "emm -> a\n",
      "mma -> .\n",
      "olivia\n",
      "... -> o\n",
      "..o -> l\n",
      ".ol -> i\n",
      "oli -> v\n",
      "liv -> i\n",
      "ivi -> a\n",
      "via -> .\n",
      "ava\n",
      "... -> a\n",
      "..a -> v\n",
      ".av -> a\n",
      "ava -> .\n"
     ]
    }
   ],
   "source": [
    "X, Y = [], []\n",
    "\n",
    "for w in words[:3]:\n",
    "    print(w)\n",
    "    context = [0] * block_size ## start with padded context of 0s\n",
    "    for ch in w + '.':\n",
    "        ix = chtoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itoch[i] for i in context), '->', itoch[ix])\n",
    "        context = context[1:] + [ix] # update the context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1]]\n",
      "\n",
      "targets:\n",
      "[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'inputs:\\n{X}\\n')\n",
    "print(f'targets:\\n{Y}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that out of the word \"emma\" we can generate 5 different example for inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3]), torch.int64, torch.Size([16]), torch.int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Embedding Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5095e-01,  6.3268e-01],\n",
       "        [ 6.7109e-01, -6.0272e-01],\n",
       "        [ 1.6476e-01, -1.6241e+00],\n",
       "        [-4.4414e-01, -2.6809e-01],\n",
       "        [ 1.9324e+00,  1.2073e+00],\n",
       "        [-9.7633e-01, -1.6983e+00],\n",
       "        [-5.4336e-01,  8.6897e-01],\n",
       "        [ 2.1105e-03,  4.4206e-01],\n",
       "        [-3.1510e-01, -1.7038e+00],\n",
       "        [ 8.8760e-01, -1.7088e+00],\n",
       "        [ 1.1976e+00,  4.5825e-01],\n",
       "        [ 3.7455e-01,  3.0131e-01],\n",
       "        [-1.2639e-01, -3.1526e-01],\n",
       "        [-7.3973e-01,  1.1733e+00],\n",
       "        [-2.5549e-01,  2.6838e-01],\n",
       "        [-5.1717e-01, -9.5439e-01],\n",
       "        [ 2.3148e+00, -6.8375e-01],\n",
       "        [-9.4651e-01,  1.0856e+00],\n",
       "        [-3.5373e-01,  1.7227e+00],\n",
       "        [-1.2434e+00, -5.8994e-01],\n",
       "        [ 2.1959e-01,  1.4281e+00],\n",
       "        [-7.1352e-01, -1.0851e-01],\n",
       "        [ 1.4134e+00,  9.6002e-01],\n",
       "        [-2.3733e+00, -4.9556e-01],\n",
       "        [-1.1405e-01, -1.9218e+00],\n",
       "        [ 1.3865e+00,  1.0302e+00],\n",
       "        [-5.4251e-01, -3.1156e+00]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn((27, 2)) # creating a 2D vector space for a 27 token vocab size\n",
    "C "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding a single integer in the embedding lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9763, -1.6983])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5] # row corresponsding to index 5 in the embedding table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27) # encoding integer 5 to a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9763, -1.6983])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output is identical. Because of all the 0s in the one-hot vector, all other values in C get masked out except for the row corresponding to the integer i.e. index 5. \n",
    "\n",
    "So the embedding of an integer can either be seen as the integer indexing into a lookup table C or it can be seen as the first layer of a larger NN. The neurons in this layer have no non-linearity and their weight matrix is C. We are encoding ints in one-hot and then feeding them to this layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting embeddings of multiple integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9763, -1.6983],\n",
       "        [-0.5434,  0.8690],\n",
       "        [ 0.0021,  0.4421]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[[5, 6, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9763, -1.6983],\n",
       "        [-0.5434,  0.8690],\n",
       "        [ 0.0021,  0.4421]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([5, 6, 7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2509,  0.6327],\n",
       "         [-0.2509,  0.6327],\n",
       "         [-0.2509,  0.6327]],\n",
       "\n",
       "        [[-0.2509,  0.6327],\n",
       "         [-0.2509,  0.6327],\n",
       "         [-0.9763, -1.6983]],\n",
       "\n",
       "        [[-0.2509,  0.6327],\n",
       "         [-0.9763, -1.6983],\n",
       "         [-0.7397,  1.1733]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also index with multi dim tensors\n",
    "C[X][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape # for everyone of the vectors in X we have a 2D embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor([ 0.6711, -0.6027])\n",
      "tensor([ 0.6711, -0.6027])\n"
     ]
    }
   ],
   "source": [
    "# example \n",
    "print(X[13, 2])\n",
    "print(C[X][13, 2])\n",
    "print(C[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so embedding all ints\n",
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing The Hidden Layer\n",
    "\n",
    "According to the paper we have 3 inputs of (in our case) 2 dims so the number of inputs for this layer is 6. Let's take the number of neurons in this layer to be 100 (upto us)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6, 100)) # weights\n",
    "B1 = torch.randn(100) # biases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to do the operation `emb @ W + b` but the embeddings are stacked in the input tensor so matmul will not work. So we need to concatenate these inputs together i.e. convert 16, 3, 2 to a 16, 6 tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[:, 0, :].shape # get everything at the 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1).shape # concatenate the seq across dim 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to generalize the above operation\n",
    "len(torch.unbind(emb, 1)) # list of tensors as in the above operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 6])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so now we can do the general operation\n",
    "torch.cat(torch.unbind(emb, 1), 1).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out there is an even better way of doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = torch.arange(18)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can represent example tensor in different sized n dim tensors\n",
    "example.view(2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.view(3, 3, 2) # product of dim numbers should be the same"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pytorch `.view` operation is extremely efficient because a tensor is stored as a 1D sequence of bytes in memory and the attributes of the tensor add the dimesionality to it. When we call `.view` we only modify these attributes but the underlying tensor remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2509,  0.6327, -0.2509,  0.6327, -0.2509,  0.6327],\n",
       "        [-0.2509,  0.6327, -0.2509,  0.6327, -0.9763, -1.6983],\n",
       "        [-0.2509,  0.6327, -0.9763, -1.6983, -0.7397,  1.1733],\n",
       "        [-0.9763, -1.6983, -0.7397,  1.1733, -0.7397,  1.1733],\n",
       "        [-0.7397,  1.1733, -0.7397,  1.1733,  0.6711, -0.6027],\n",
       "        [-0.2509,  0.6327, -0.2509,  0.6327, -0.2509,  0.6327],\n",
       "        [-0.2509,  0.6327, -0.2509,  0.6327, -0.5172, -0.9544],\n",
       "        [-0.2509,  0.6327, -0.5172, -0.9544, -0.1264, -0.3153],\n",
       "        [-0.5172, -0.9544, -0.1264, -0.3153,  0.8876, -1.7088],\n",
       "        [-0.1264, -0.3153,  0.8876, -1.7088,  1.4134,  0.9600],\n",
       "        [ 0.8876, -1.7088,  1.4134,  0.9600,  0.8876, -1.7088],\n",
       "        [ 1.4134,  0.9600,  0.8876, -1.7088,  0.6711, -0.6027],\n",
       "        [-0.2509,  0.6327, -0.2509,  0.6327, -0.2509,  0.6327],\n",
       "        [-0.2509,  0.6327, -0.2509,  0.6327,  0.6711, -0.6027],\n",
       "        [-0.2509,  0.6327,  0.6711, -0.6027,  1.4134,  0.9600],\n",
       "        [ 0.6711, -0.6027,  1.4134,  0.9600,  0.6711, -0.6027]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(16, 6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can get the hidden states of the hidden layer by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = emb.view(16, 6) @ W1 + B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 100])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're still hardcoding emb's dimensionality so to fix that we can do\n",
    "h = emb.view(-1, 6) @ W1 + B1 # -1 means pytorch can derive the appropriate dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we perform a `tanh` (from the paper) to get the 100 dimensional activations of all of our 16 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(-1, 6) @ W1 + B1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing The Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27)) # input is 100 and output number of neurons is 27 for our vocab size\n",
    "B2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 27])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 27])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting prob dist from logits\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5913e-06, 2.0015e-06, 1.7259e-09, 3.1433e-03, 1.4249e-12, 3.9630e-02,\n",
       "        2.3779e-08, 1.7425e-07, 1.4293e-09, 9.1998e-01, 8.2286e-03, 7.5289e-09,\n",
       "        2.4810e-07, 2.4319e-02, 2.4521e-11, 6.1648e-08, 2.4133e-03, 6.8097e-07,\n",
       "        8.1947e-09, 1.5347e-10, 5.5814e-05, 5.8947e-06, 5.1442e-04, 1.6971e-03,\n",
       "        6.1583e-07, 6.3879e-08, 1.1128e-05])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9630e-02, 3.5144e-09, 1.3506e-04, 7.9603e-07, 5.3318e-15, 6.1648e-08,\n",
       "        4.9938e-08, 1.9571e-05, 3.8193e-16, 3.0336e-09, 4.4803e-06, 2.1925e-05,\n",
       "        2.0015e-06, 6.8455e-11, 5.8309e-08, 6.7171e-07])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterate through the rows of probs\n",
    "probs[torch.arange(16), Y] # probs assigned by this NN to the correct character in the seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.7703)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlll = - probs[torch.arange(16), Y].log().mean() # negative log likelihood loss\n",
    "nlll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
